\section{Independence Models}

\begin{frame}{Statistical Models}
    \begin{block}{Definition, \cite{}}
    A \emph{statistical model} $\mc{P}$ is the collection of probability distributions, usually parameterised by a function  called a \emph{parametrisation}
    $$ \Theta \ra \mc{P}, \quad \text{given by} \quad \theta \mapsto P_{\theta},\quad \text{so that}\quad \mc{P} = \{ P_{\theta} : \theta \in \Theta \}, $$
    where $\Theta$ is the \emph{parameter space}. $\Theta$ is usually a subset of $\RR^{n}$.
    \end{block}

    \begin{block}{McCullagh, 2002, \cite{PM2002}}
    This should be defined using category theory.
    \end{block}

\end{frame}

\begin{frame}{Contingency Tables}
    
    \begin{itemize}
        \item Classify using two criteria with $r$ and $c$ levels, yielding two random variables $X$ and $Y$. 
        \item Note outcomes as $[r] := \{1,\ldots, r\}$, and $[c] := \{ 1, \ldots, c \}$.
    \end{itemize}
    
    All information about $X$ and $Y$ is contained in the \emph{joint probabilities}:
    $$ p_{ij} = P(X = i; Y = j), \quad i \in [r],\ j \in [c]. $$

    \begin{itemize}
        \item These in turn determine the \emph{marginal probabilities}:
    \end{itemize}
    \vspace*{-12pt}
    \begin{equation*}
        \begin{split}
            p_{i+} &:= \sum_{j = 1}^{c} p_{ij} = P(X = i), \quad i \in [r], \\
            p_{+j} &:= \sum_{i = 1}^{r} p_{ij} = P(Y = j), \quad j \in [c].
        \end{split}
    \end{equation*}

\end{frame}

\begin{frame}
    \begin{block}{Definition}
    \begin{itemize}
        \item Two random variables $X$ and $Y$ are \emph{independent} if the joint probabilities factor as $p_{ij} = p_{i+}\cdot p_{+j}$, for all $i \in [r]$ and $j \in [c]$. 
        \item Denote independence of $X$ and $Y$ by $X \indep Y$.
    \end{itemize}
    \end{block}

    \begin{block}{Proposition}
        \emph{Two random variables $X$ and $Y$ are independent if and only if the $(r \times c)$-matrix, $p = (p_{ij})$, has rank one.}
    \end{block}

    For a $(2 \times 2)$-table, we thus have:

\begin{center}
\begin{tabular}{lcc}
        & $P(Y=1)$ & $P(Y=2)$\\\hline
$P(X=1)$ & $p_{11}$ & $p_{12}$\\
$P(X=2)$ & $p_{21}$ & $p_{22}$\\\hline
\end{tabular}
$ \ \overset{X \indep Y}{\longleadsto{1.25}} \ p_{11}p_{22} = p_{12}p_{21}. $
\end{center}
\end{frame}

\begin{frame}{Finally Some Geometry}

    Suppose now we select $n$ cases, giving rise to $n$ independent pairs of discrete random variables:
    $$ \begin{pmatrix} X^{(1)} \\ Y^{(1)} \end{pmatrix}, \begin{pmatrix} X^{(2)} \\ Y^{(2)} \end{pmatrix}, \ldots, \begin{pmatrix} X^{(n)} \\ Y^{(n)} \end{pmatrix}, $$
    all drawn from the same distribution, i.e.:
    $$ P( X^{(k)} = i; Y^{(k)} = j ) = p_{ij}, \quad \text{for all } i \in [r],\ j \in [c],\ k \in [n]. $$

    \begin{block}{Probability Simplices}
    Joint probability matrix $p = (p_{ij})$ is an \emph{unknown} element of the $(rc-1)$-dimensional \emph{probability simplex}:
    $$ \Delta_{rc-1} = \Set{ q \in \RR^{r\times c} | q_{ij} \geq 0, \text{ for all } i,j, \text{ and } \sum_{i = 1}^{r}\sum_{j = 1}^{c} q_{ij} = 1 }. $$
    \end{block}

\end{frame}

\begin{frame}

    \begin{block}{Definitions}
        \begin{itemize}
        \item A \emph{statistical model} $\mc{M}$ is a subset of $\Delta_{rc-1}$. It represents the set of all candidates for the unknown distribution $p$.

        \item The \emph{independence model} for $X$ and $Y$ is the set
        $$ \mc{M}_{X \indep Y} := \Set{ p \in \Delta_{rc - 1} | \rank(p) = 1 }. $$
        \end{itemize}
    \end{block}

    $\mc{M}_{X \indep Y}$ is the intersection of $\Delta_{rc-1}$ and the set of all matrices $p = (p_{ij})$ such that
    $$ p_{ij}p_{kl} - p_{il}p_{jk} = 0, \quad (1 \leq i < k \leq r, \text{ and } 1 \leq j < l \leq c). $$

    These are called \emph{Segre varieties} in algebraic geometry.

\end{frame}
